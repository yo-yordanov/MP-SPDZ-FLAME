# coding: latin-1

from Compiler.types import sint, sfix, regint, Array, Matrix, MemValue, cint, cfix
from Compiler.library import (
    listen_for_clients,
    accept_client_connection,
    crash,
    print_ln,
    do_while,
    for_range,
    for_range_multithread,
    if_,
)
from Compiler.instructions import closeclientconnection
from Compiler.mpc_math import sqrt, log_fx, cos

import math

PORTNUM = 14000
N_THREADS = 2
N_ITERATIONS = 1
MAX_CLIENTS = 4
MODEL_SIZE = 5
PRECISION = 32

sfix.set_precision(PRECISION)
cfix.set_precision(PRECISION)

EPSILON = MemValue(cfix(0.6))
DELTA = MemValue(sfix(1e-5))
MIN_PTS = MemValue(cint(MAX_CLIENTS // 2 + 1))
LEARNING_RATE = MemValue(sfix(0.1))

if len(program.args) > 1:
    program.active = bool(int(program.args[2]))


def accept_client():
    """
    Accept a client connection and read the client id and last flag.
    Returns:
        client_socket_id (int): The socket id of the connected client.
        last (int): A flag indicating if this is the last client.
    """
    client_socket_id = accept_client_connection(PORTNUM)
    last = regint.read_from_socket(client_socket_id)
    return client_socket_id, last


def accept_clients():
    """
    Accept clients until the maximum number of clients is reached or last client is connected.
    Returns:
        n_clients (MemValue): The number of clients that have connected.
    """
    # Clients socket id (integer).
    sockets = Array(MAX_CLIENTS, regint)
    # Number of clients
    n_clients = MemValue(regint(0))
    # Client ids to identity client
    client_ids = Array(MAX_CLIENTS, sint)
    # Keep track of received inputs
    seen = Array(MAX_CLIENTS, regint)
    seen.assign_all(0)

    @do_while
    def _():
        cid, last = accept_client()

        @if_(cid >= MAX_CLIENTS)
        def _():
            print_ln("client id too high")
            crash()

        sockets[cid] = cid
        client_ids[cid] = cid
        seen[cid] = 1

        @if_(last == 1)
        def _():
            n_clients.write(cid + 1)

        return (sum(seen) < n_clients) + (n_clients == 0)

    return n_clients


def close_connections(number_clients):
    """
    Close all client connections.
    Args:
        number_clients (int): The number of clients that are connected.
    """

    @for_range(number_clients)
    def _(i):
        closeclientconnection(i)


def client_input(n_clients, n_threads=N_THREADS, n_parallel=1):
    """
    Receive input vectors from clients.
    Args:
        n_clients (int): The actual number of clients.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        client_values (sfix.Matrix): A matrix containing the input vectors from the clients.
    """
    client_values = Matrix(MAX_CLIENTS, MODEL_SIZE, sfix)

    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(client_id):
        client_values[client_id] = sfix.receive_from_client(MODEL_SIZE, client_id)

    return client_values


def compute_l2_norms(n_clients, values, n_threads=N_THREADS, n_parallel=1):
    """
    Compute the L2 norms of the input vectors for each client.
    Args:
        n_clients (int): The actual number of clients.
        values (sfix.Matrix): The input vectors from the clients.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        sfix.Array: An array containing the L2 norms for each client.
    """
    norms = Array(MAX_CLIENTS, sfix)

    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(cid):
        norm = values[cid].dot(values[cid])[0]
        norms[cid] = sqrt(norm)

    return norms


def compute_cosine_distances(
    n_clients, values, norms, n_threads=N_THREADS, n_parallel=1
):
    """
    Compute the cosine distances between client input vectors.
    Args:
        n_clients (int): The actual number of clients.
        values (sfix.Matrix): The input vectors from the clients.
        norms (sfix.Array): The L2 norms of the input vectors.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        sfix.Matrix: A matrix containing the cosine distances for each client pair.
    """
    cos_distances = Matrix(MAX_CLIENTS, MAX_CLIENTS, sfix).assign_all(sfix(0))

    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(i):
        @for_range(i + 1, n_clients)
        def _(j):
            cos_dist = sfix(1) - values[i].dot(values[j])[0] / (norms[i] * norms[j])
            cos_distances[i][j] = cos_distances[j][i] = cos_dist

    return cos_distances


def dbscan_init(t, n_clients, cosine_distances, n_threads=N_THREADS, n_parallel=1):
    """
    Initialize the DBSCAN algorithm variables.
    Args:
        t (type): The type to use for the arrays (sint or cfix).
        n_clients (int): The actual number of clients.
        cosine_distances (sfix.Matrix): The matrix of cosine distances between clients.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        tuple: A tuple containing initialized arrays for labels, visited, neighbors, neighbor_counts, and cluster_id.
    """
    labels = Array(MAX_CLIENTS, t).assign_all(t(0))
    visited = Array(MAX_CLIENTS, t).assign_all(t(0))
    neighbors = Matrix(MAX_CLIENTS, MAX_CLIENTS, t).assign_all(t(0))
    neighbor_counts = Array(MAX_CLIENTS, t).assign_all(t(0))

    # Precompute neighbors matrix and neighbor_counts array using multithreading
    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(i):
        @for_range(n_clients)
        def _(j):
            is_neighbor = (i != j) * (cosine_distances[i][j] < EPSILON)
            neighbors[i][j] = is_neighbor
            neighbor_counts[i] = neighbor_counts[i] + is_neighbor

    # Use MemValue for cluster_id to handle modifications properly
    cluster_id = MemValue(t(0))

    return labels, visited, neighbors, neighbor_counts, cluster_id


def dbscan(n_clients, cosine_distances, n_threads=N_THREADS, n_parallel=1):
    """
    Perform the DBSCAN clustering algorithm on the revealed cosine distances.
    Args:
        n_clients (int): The actual number of clients.
        cosine_distances (sfix.Matrix): The matrix of cosine distances between clients.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        cint.Array: An array containing the cluster labels for each client.
    """
    # Initialize DBSCAN parameters
    labels, visited, neighbors, neighbor_counts, cluster_id = dbscan_init(
        cint, n_clients, cosine_distances, n_threads, n_parallel
    )

    @for_range(n_clients)
    def _(i):
        # Check if point should start a new cluster
        should_expand = (visited[i] == 0) * (neighbor_counts[i] >= MIN_PTS)

        @if_(should_expand)
        def expand_cluster():
            visited[i] = cint(1)
            current_cluster = cluster_id.read() + 1
            cluster_id.write(current_cluster)

            # Initialize queue for BFS
            queue = cint.Array(MAX_CLIENTS)
            queue_head = MemValue(cint(0))
            queue_tail = MemValue(cint(1))
            in_queue = cint.Array(MAX_CLIENTS).assign_all(cint(0))

            # Add starting point to queue
            queue[0] = i
            in_queue[i] = cint(1)
            labels[i] = current_cluster

            # Process queue
            @for_range(n_clients)  # Upper bound for queue processing
            def _(iteration):
                head = queue_head.read()
                tail = queue_tail.read()

                # Check if queue is not empty
                @if_(head < tail)
                def process_queue_item():
                    current_point = queue[head]
                    queue_head.write(head + 1)

                    # Check all neighbors of current point
                    @for_range(n_clients)
                    def _(j):
                        is_neighbor = neighbors[current_point][j]
                        not_labeled = labels[j] == 0
                        not_in_queue = in_queue[j] == 0

                        # Label neighbor if it's unlabeled
                        should_label = is_neighbor * not_labeled
                        labels[j] = labels[j] + should_label * current_cluster

                        # Mark as visited if it's a neighbor
                        visited[j] = visited[j] + is_neighbor * (1 - visited[j])

                        # Add to queue if it's a core point and not already in queue
                        is_core = neighbor_counts[j] >= MIN_PTS
                        should_add_to_queue = should_label * is_core * not_in_queue

                        # Oblivious queue addition
                        current_tail = queue_tail.read()

                        @if_(should_add_to_queue)
                        def add_to_queue():
                            queue[current_tail] = j
                            in_queue[j] = cint(1)
                            queue_tail.write(current_tail + 1)

    return labels


def compute_median(n_clients, norms):
    """
    Compute the clipping parameters for the input vectors.
    Args:
        n_clients (int): The actual number of clients.
        norms (sfix.Array): The L2 norms of the input vectors.
        lr (sfix): The learning rate to scale the median.
    Returns:
        MemValue: The median value of the L2 norms, scaled by the learning rate.
    """
    sorted_norms = Array(MAX_CLIENTS, sfix).assign(norms)
    sorted_norms.sort()
    median_index = (n_clients) // 2

    odd_median = sorted_norms[median_index]
    even_median = (sorted_norms[median_index - 1] + sorted_norms[median_index]) / 2

    return MemValue((n_clients % 2 == 1).if_else(odd_median, even_median))


def clip_updates(
    n_clients,
    client_values,
    norms,
    median,
    labels,
    n_threads=N_THREADS,
    n_parallel=1,
):
    """
    Clip gradients based on the clipping bound and aggregate them.
    Args:
        n_clients (int): The actual number of clients.
        client_values (sfix.Matrix): The input vectors from clients.
        norms (sfix.Array): The L2 norms of the input vectors.
        median (sfix): The clipping bound value.
        labels (cint.Array): The cluster labels for each client.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        sfix.Array: The aggregated global model update.
    """
    clipped_updates = Matrix(MAX_CLIENTS, MODEL_SIZE, sfix)

    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(i):
        clipping_parameter = median / norms[i]
        clipping_parameter = (clipping_parameter < 1).if_else(
            clipping_parameter, sfix(1)
        )
        clipped_updates[i] = client_values[i] * clipping_parameter * labels[i]

    return clipped_updates


def aggregate_updates(n_clients, clipped_updates, labels):
    """
    Aggregate the clipped updates based on cluster labels.
    Args:
        n_clients (int): The actual number of clients.
        clipped_updates (sfix.Matrix): The clipped updates from clients.
        labels (cint.Array): The cluster labels for each client.
    Returns:
        sfix.Array: The aggregated global model update.
    """
    aggregated_update = Array(MODEL_SIZE, sfix).assign_all(sfix(0))
    honest_updates = MemValue(sfix(0))

    @for_range(n_clients)
    def _(i):
        aggregated_update[:] += clipped_updates[i][:]
        honest_updates.write(honest_updates.read() + labels[i])

    return (honest_updates.read() > 0).if_else(
        aggregated_update / honest_updates.read(), aggregated_update
    )


def calculate_noise_level(t, median, epsilon=EPSILON, delta=DELTA):
    """
    Calculate σ = λ.St where λ = (1/ε).√(2ln(1.25/δ))
    Args:
        sensitivity (float): The sensitivity of the function.
        epsilon (float): The privacy budget.
        delta (float): The failure probability.
    Returns:
        float: The noise level σ.
    """
    # Calculate λ = (1/ε) · √(2ln(1.25/δ))
    ln_term = log_fx(t(1.25) / delta, math.e)
    sqrt_term = sqrt(t(2) * ln_term)
    lambda_val = (t(1.0) / epsilon) * sqrt_term

    # Calculate σ = λ · St
    return lambda_val * median


def compute_adaptive_noise(adaptive_noising_level):
    """
    Compute adaptive noise based on the mean and standard deviation.
    Args:
        mean (float): The mean of the noise distribution.
        std_dev (float): The standard deviation of the noise distribution.
    Returns:
        float: The computed noise value.
    """
    u1 = sfix.get_random(0.001, 0.999)
    u2 = sfix.get_random(0.001, 0.999)

    z0 = sqrt(-2 * log_fx(u1, math.e)) * cos(2 * math.pi * u2)
    return adaptive_noising_level * z0


def add_noise_to_update(n_clients, aggregated_update, adaptive_noise):
    """
    Add adaptive noise to the aggregated model update.
    Args:
        aggregated_update (sfix.Array): The aggregated global model update.
        adaptive_noise (sfix): The adaptive noise level.
    Returns:
        sfix.Array: The updated global model with added noise.
    """
    noised_update = Array(MODEL_SIZE, sfix).assign(aggregated_update)

    @for_range(n_clients)
    def _(i):
        noised_update[i] += compute_adaptive_noise(adaptive_noise)

    return noised_update


def main():
    # Start listening for client socket connections
    listen_for_clients(PORTNUM)
    print_ln("Listening for client connections on base port %s", PORTNUM)

    def iteration(_=None):
        print_ln("Starting a new iteration.")

        # Accept clients and read their inputs.
        n_clients = accept_clients()
        client_values = client_input(n_clients)
        # print_ln("Client values: %s", client_values.reveal_nested())

        # Compute L2 norm of each client's input.
        norms = compute_l2_norms(n_clients, client_values)
        # print_ln("L2 norms: %s", norms.reveal_list())

        # Compute pairwise cosine distances.
        cosine_distances = compute_cosine_distances(n_clients, client_values, norms)
        revealed_cosine_distances = cosine_distances.reveal()
        # print_ln("Cosine distances: %s", revealed_cosine_distances)

        # Cluster cosine similarities using DBSCAN.
        labels = dbscan(n_clients, revealed_cosine_distances)
        # print_ln("Labels: %s", labels.reveal())

        # Compute clipping bound (scaled median of L2 norms, since the global model is 0^n).
        median = compute_median(n_clients, norms)
        # print_ln("Median: %s", median.reveal())

        # Clip gradients and aggregate global model update.
        clipped_updates = clip_updates(n_clients, client_values, norms, median, labels)
        # print_ln("Clipped updates: %s", clipped_updates.reveal())
        aggregated_update = aggregate_updates(n_clients, clipped_updates, labels)
        # print_ln("Aggregated update: %s", aggregated_update.reveal())

        # Add adaptive noise to the model updates.
        noise_level = calculate_noise_level(sfix, median)
        # print_ln("Noise level: %s", noise_level.reveal())
        noised_update = add_noise_to_update(n_clients, aggregated_update, noise_level)
        revealed_noised_update = noised_update.reveal()
        print_ln("Noisy aggregated update: %s", revealed_noised_update)

        close_connections(n_clients)

        return True

    if N_ITERATIONS > 0:
        print("run %d iterations" % N_ITERATIONS)
        for_range(N_ITERATIONS)(iteration)
    else:
        print("run forever")
        do_while(iteration)


main()
