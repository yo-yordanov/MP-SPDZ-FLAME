# coding: latin-1

from Compiler.types import sint, sfix, regint, Array, Matrix, MemValue, cint, cfix
from Compiler.library import (
    listen_for_clients,
    accept_client_connection,
    crash,
    print_ln,
    do_while,
    for_range,
    for_range_multithread,
    if_,
)
from Compiler.instructions import closeclientconnection
from Compiler.mpc_math import sqrt, log_fx, cos

import math

PORTNUM = 14000
N_THREADS = 2
N_ITERATIONS = 1
MAX_CLIENTS = 4
MODEL_SIZE = 5
PRECISION = 32

sfix.set_precision(PRECISION)
cfix.set_precision(PRECISION)

EPSILON = MemValue(cfix(0.3))
DELTA = MemValue(sfix(1e-5))
MIN_PTS = MemValue(cint(MAX_CLIENTS // 2 + 1))
LEARNING_RATE = MemValue(sfix(0.1))

if len(program.args) > 1:
    program.active = bool(int(program.args[2]))


def accept_client():
    """
    Accept a client connection and read the client id and last flag.
    Returns:
        client_socket_id (int): The socket id of the connected client.
        last (int): A flag indicating if this is the last client.
    """
    client_socket_id = accept_client_connection(PORTNUM)
    last = regint.read_from_socket(client_socket_id)
    return client_socket_id, last


def accept_clients():
    """
    Accept clients until the maximum number of clients is reached or last client is connected.
    Returns:
        n_clients (MemValue): The number of clients that have connected.
    """
    # Clients socket id (integer).
    sockets = Array(MAX_CLIENTS, regint)
    # Number of clients
    n_clients = MemValue(regint(0))
    # Client ids to identity client
    client_ids = Array(MAX_CLIENTS, sint)
    # Keep track of received inputs
    seen = Array(MAX_CLIENTS, regint)
    seen.assign_all(0)

    @do_while
    def _():
        cid, last = accept_client()

        @if_(cid >= MAX_CLIENTS)
        def _():
            print_ln("client id too high")
            crash()

        sockets[cid] = cid
        client_ids[cid] = cid
        seen[cid] = 1

        @if_(last == 1)
        def _():
            n_clients.write(cid + 1)

        return (sum(seen) < n_clients) + (n_clients == 0)

    return n_clients


def close_connections(number_clients):
    """
    Close all client connections.
    Args:
        number_clients (int): The number of clients that are connected.
    """

    @for_range(number_clients)
    def _(i):
        closeclientconnection(i)


def client_input(n_clients, n_threads=N_THREADS, n_parallel=1):
    """
    Receive input vectors from clients.
    Args:
        n_clients (int): The actual number of clients.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        client_values (sfix.Matrix): A matrix containing the input vectors from the clients.
    """
    client_values = Matrix(MAX_CLIENTS, MODEL_SIZE, sfix)

    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(client_id):
        client_values[client_id] = sfix.receive_from_client(MODEL_SIZE, client_id)

    return client_values


def compute_euclidean_distances(
    n_clients, client_values, global_model, n_threads=N_THREADS, n_parallel=1
):
    """
    Compute the Euclidean distances between two sets of client input vectors.
    Args:
        n_clients (int): The actual number of clients.
        client_values (sfix.Matrix): The set of input vectors from the clients.
        global_model (sfix.Array): The global model vector.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        sfix.Array: A matrix containing the Euclidean distances for each client and the global model.
    """
    distances = Array(MAX_CLIENTS, sfix).assign_all(sfix(0))

    @for_range(n_clients)
    def _(i):
        diff = Array(MODEL_SIZE, sfix).assign(client_values[i] - global_model)
        # print_ln("Client diff: %s", diff.reveal())
        dist = diff.dot(diff)[0]
        # print_ln("Client distance: %s", dist.reveal())
        distances[i] = sqrt(dist)
        # print_ln("Client Euclidean distance: %s", distances[i].reveal())

    return distances


def compute_l2_norms(n_clients, client_values, n_threads=1, n_parallel=1):
    """
    Compute the L2 norms of the input vectors for each client.
    Args:
        n_clients (int): The actual number of clients.
        client_values (sfix.Matrix): The input vectors from the clients.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        sfix.Array: An array containing the L2 norms for each client.
    """
    norms = Array(MAX_CLIENTS, sfix)

    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(cid):
        norm = client_values[cid].dot(client_values[cid])[0]
        norms[cid] = sqrt(norm)

    return norms


def compute_cosine_distances(
    n_clients, client_values, norms, n_threads=1, n_parallel=1
):
    """
    Compute the cosine distances between client input vectors.
    Args:
        n_clients (int): The actual number of clients.
        client_values (sfix.Matrix): The input vectors from the clients.
        norms (sfix.Array): The L2 norms of the input vectors.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        sfix.Matrix: A matrix containing the cosine distances for each client pair.
    """
    cos_distances = Matrix(MAX_CLIENTS, MAX_CLIENTS, sfix)

    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(i):
        @for_range(i, n_clients)
        def _(j):
            cos_dist = sfix(1) - client_values[i].dot(client_values[j])[0] / (
                norms[i] * norms[j]
            )
            cos_distances[i][j] = cos_distances[j][i] = cos_dist

    return cos_distances


def dbscan_init(
    t,
    n_clients,
    cosine_distances,
    n_threads=1,
    n_parallel=1,
):
    """
    Initialize the DBSCAN algorithm variables.
    Args:
        t (type): The type to use for the arrays (sint or cfix).
        n_clients (int): The actual number of clients.
        cosine_distances (sfix.Matrix): The matrix of cosine distances between clients.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        tuple: A tuple containing initialized arrays for labels, visited, neighbors, neighbor_counts, and cluster_id.
    """
    labels = Array(MAX_CLIENTS, t).assign_all(t(-1))
    visited = Array(MAX_CLIENTS, t).assign_all(t(0))
    neighbors = Matrix(MAX_CLIENTS, MAX_CLIENTS, t).assign_all(t(0))
    neighbor_counts = Array(MAX_CLIENTS, t).assign_all(t(0))

    # Precompute neighbors matrix and neighbor_counts array using multithreading
    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(i):
        @for_range(n_clients)
        def _(j):
            is_neighbor = cosine_distances[i][j] < EPSILON
            neighbors[i][j] = is_neighbor
            neighbor_counts[i] = neighbor_counts[i] + is_neighbor

    # Use MemValue for cluster_id to handle modifications properly
    cluster_id = MemValue(t(-1))

    return labels, visited, neighbors, neighbor_counts, cluster_id


def queue_init(t, start_point):
    """
    Initialize the queue for BFS in the DBSCAN algorithm.
    Args:
        t (type): The type to use for the queue (sint or cfix).
        start_point (int): The index of the starting point for the cluster expansion.
    Returns:
        tuple: A tuple containing the initialized queue array, queue_head, queue_tail, and in_queue array.
    """
    queue = t.Array(MAX_CLIENTS)
    queue_head = MemValue(t(0))
    queue_tail = MemValue(t(1))
    in_queue = t.Array(MAX_CLIENTS).assign_all(t(0))

    # Add starting point to queue
    queue[0] = start_point
    in_queue[start_point] = t(1)

    return queue, queue_head, queue_tail, in_queue


def process_queue_item(
    n_clients,
    labels,
    visited,
    neighbors,
    neighbor_counts,
    current_cluster,
    queue,
    queue_head,
    queue_tail,
    in_queue,
):
    """
    Process an item from the queue in the DBSCAN algorithm.
    Args:
        n_clients (int): The actual number of clients.
        labels (cint.Array): The array of cluster labels.
        visited (cint.Array): The array indicating whether a point has been visited.
        neighbors (cint.Matrix): The matrix indicating neighbor relationships.
        neighbor_counts (cint.Array): The array of neighbor counts for each point.
        current_cluster (int): The current cluster ID being expanded.
        queue (cint.Array): The queue of points to process.
        queue_head (MemValue): The head index of the queue.
        queue_tail (MemValue): The tail index of the queue.
        in_queue (cint.Array): The array indicating whether a point is in the queue.
    """
    current_point = queue[queue_head.read()]
    queue_head.write(queue_head.read() + 1)

    # Check all neighbors of current point
    @for_range(n_clients)
    def _(j):
        is_neighbor = neighbors[current_point][j]
        not_labeled = labels[j] == -1
        is_core = neighbor_counts[j] >= MIN_PTS
        not_in_queue = in_queue[j] == 0

        # Label neighbor if it's unlabeled
        @if_(is_neighbor * not_labeled)
        def label_neighbor():
            labels[j] = current_cluster

        # Mark as visited if it's a neighbor
        @if_(is_neighbor)
        def mark_visited():
            visited[j] = cint(1)

        # Add to queue if it's a core point and not already in queue
        @if_(is_neighbor * not_labeled * is_core * not_in_queue)
        def add_to_queue():
            current_tail = queue_tail.read()
            queue[current_tail] = j
            in_queue[j] = cint(1)
            queue_tail.write(current_tail + 1)


def dbscan(
    n_clients,
    cosine_distances,
    n_threads=1,
    n_parallel=1,
):
    """
    Perform the DBSCAN clustering algorithm on the revealed cosine distances.
    Args:
        n_clients (int): The actual number of clients.
        cosine_distances (sfix.Matrix): The matrix of cosine distances between clients.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        cint.Array: An array containing the cluster labels for each client.
    """
    # Initialize DBSCAN parameters
    labels, visited, neighbors, neighbor_counts, cluster_id = dbscan_init(
        cint,
        n_clients,
        cosine_distances,
        n_threads,
        n_parallel,
    )

    @for_range(n_clients)
    def _(i):
        # Check if point should start a new cluster
        should_expand = (visited[i] == 0) * (neighbor_counts[i] >= MIN_PTS)

        @if_(should_expand)
        def expand_cluster():
            visited[i] = cint(1)
            current_cluster = cluster_id.read() + 1
            cluster_id.write(current_cluster)

            # Initialize queue for BFS and add the current point
            queue, queue_head, queue_tail, in_queue = queue_init(cint, i)
            labels[i] = current_cluster

            # Process queue
            @for_range(n_clients)  # Upper bound for queue processing
            def _(iteration):
                head = queue_head.read()
                tail = queue_tail.read()

                # Check if queue is not empty
                @if_(head < tail)
                def _():
                    process_queue_item(
                        n_clients,
                        labels,
                        visited,
                        neighbors,
                        neighbor_counts,
                        current_cluster,
                        queue,
                        queue_head,
                        queue_tail,
                        in_queue,
                    )

    return labels


def compute_median(n_clients, norms):
    """
    Compute the clipping parameters for the input vectors.
    Args:
        n_clients (int): The actual number of clients.
        norms (sfix.Array): The L2 norms of the input vectors.
        lr (sfix): The learning rate to scale the median.
    Returns:
        MemValue: The median value of the L2 norms, scaled by the learning rate.
    """
    sorted_norms = Array(MAX_CLIENTS, sfix).assign(norms)
    sorted_norms.sort()
    median_index = (n_clients) // 2

    odd_median = sorted_norms[median_index]
    even_median = (sorted_norms[median_index - 1] + sorted_norms[median_index]) / 2

    return MemValue((n_clients % 2 == 1).if_else(odd_median, even_median))


def clip_updates(
    n_clients,
    client_values,
    norms,
    median,
    labels,
    n_threads=N_THREADS,
    n_parallel=1,
):
    """
    Clip gradients based on the clipping bound and aggregate them.
    Args:
        n_clients (int): The actual number of clients.
        client_values (sfix.Matrix): The input vectors from clients.
        norms (sfix.Array): The L2 norms of the input vectors.
        median (sfix): The clipping bound value.
        labels (cint.Array): The cluster labels for each client.
        n_threads (int): The number of threads to use for parallel processing.
        n_parallel (int): The number of parallel operations to perform.
    Returns:
        sfix.Array: The aggregated global model update.
    """
    clipped_updates = Matrix(MAX_CLIENTS, MODEL_SIZE, sfix).assign_all(sfix(0))

    @for_range_multithread(n_threads, n_parallel, n_clients)
    def _(i):
        clipping_parameter = median / norms[i]
        clipping_parameter = (clipping_parameter < 1).if_else(
            clipping_parameter, sfix(1)
        )

        @if_(labels[i] >= 0)
        def _():
            clipped_updates[i] = client_values[i] * clipping_parameter

    return clipped_updates


def aggregate_updates(n_clients, clipped_updates, labels):
    """
    Aggregate the clipped updates based on cluster labels.
    Args:
        n_clients (int): The actual number of clients.
        clipped_updates (sfix.Matrix): The clipped updates from clients.
        labels (cint.Array): The cluster labels for each client.
    Returns:
        sfix.Array: The aggregated global model update.
    """
    aggregated_update = Array(MODEL_SIZE, sfix).assign_all(sfix(0))
    honest_updates = MemValue(sfix(0))

    @for_range(n_clients)
    def _(i):
        aggregated_update[:] += clipped_updates[i][:]
        honest_updates.write(honest_updates.read() + labels[i])

    return (honest_updates.read() > 0).if_else(
        aggregated_update / honest_updates.read(), aggregated_update
    )


def calculate_noise_level(t, median, epsilon=EPSILON, delta=DELTA):
    """
    Calculate σ = λ.St where λ = (1/ε).√(2ln(1.25/δ))
    Args:
        sensitivity (float): The sensitivity of the function.
        epsilon (float): The privacy budget.
        delta (float): The failure probability.
    Returns:
        float: The noise level σ.
    """
    # Calculate λ = (1/ε) · √(2ln(1.25/δ))
    ln_term = log_fx(t(1.25) / delta, math.e)
    sqrt_term = sqrt(t(2) * ln_term)
    lambda_val = (t(1.0) / epsilon) * sqrt_term

    # Calculate σ = λ · St
    return lambda_val * median


def compute_adaptive_noise(adaptive_noising_level):
    """
    Compute adaptive noise based on the mean and standard deviation.
    Args:
        mean (float): The mean of the noise distribution.
        std_dev (float): The standard deviation of the noise distribution.
    Returns:
        float: The computed noise value.
    """
    u1 = sfix.get_random(0.001, 0.999)
    u2 = sfix.get_random(0.001, 0.999)

    z0 = sqrt(-2 * log_fx(u1, math.e)) * cos(2 * math.pi * u2)
    return adaptive_noising_level * z0


def add_noise_to_update(n_clients, aggregated_update, adaptive_noise):
    """
    Add adaptive noise to the aggregated model update.
    Args:
        aggregated_update (sfix.Array): The aggregated global model update.
        adaptive_noise (sfix): The adaptive noise level.
    Returns:
        sfix.Array: The updated global model with added noise.
    """
    noised_update = Array(MODEL_SIZE, sfix).assign(aggregated_update)

    @for_range(n_clients)
    def _(i):
        noised_update[i] += compute_adaptive_noise(adaptive_noise)

    return noised_update


def main():
    """
    Main function to run the FLAME protocol.
    It listens for client connections, accepts inputs, computes L2 norms,
    computes cosine distances, clusters the inputs using DBSCAN,
    computes clipping bounds, clips gradients, aggregates updates,
    and adds adaptive noise to the global model update.
    """
    # Start listening for client socket connections
    listen_for_clients(PORTNUM)
    print_ln("Listening for client connections on base port %s", PORTNUM)

    def iteration(_=None):
        print_ln("Starting a new iteration.")

        # Accept clients and read their inputs.
        n_clients = accept_clients()
        client_values = client_input(n_clients)
        # print_ln("Client values: %s", client_values.reveal_nested())

        # Compute L2 norm of each client's input.
        norms = compute_l2_norms(n_clients, client_values)

        # Compute pairwise cosine distances.
        cosine_distances = compute_cosine_distances(n_clients, client_values, norms)
        revealed_cosine_distances = cosine_distances.reveal()
        # print_ln("Cosine distances: %s", revealed_cosine_distances)

        # Cluster cosine similarities using DBSCAN.
        labels = dbscan(n_clients, revealed_cosine_distances)
        # print_ln("Labels: %s", labels.reveal())

        # Compute clipping bound (scaled median of L2 norms, since the global model is 0^n).
        median = compute_median(n_clients, norms)
        # print_ln("Median: %s", median.reveal())

        # Clip gradients and aggregate global model update.
        clipped_updates = clip_updates(n_clients, client_values, norms, median, labels)
        # print_ln("Clipped updates: %s", clipped_updates.reveal())
        aggregated_update = aggregate_updates(n_clients, clipped_updates, labels)
        # print_ln("Aggregated update: %s", aggregated_update.reveal())

        # Add adaptive noise to the model updates.
        noise_level = calculate_noise_level(sfix, median)
        # print_ln("Noise level: %s", noise_level.reveal())
        noised_update = add_noise_to_update(n_clients, aggregated_update, noise_level)
        revealed_noised_update = noised_update.reveal()
        print_ln("Global model update: %s", revealed_noised_update)

        close_connections(n_clients)

        return True

    if N_ITERATIONS > 0:
        print("run %d iterations" % N_ITERATIONS)
        for_range(N_ITERATIONS)(iteration)
    else:
        print("run forever")
        do_while(iteration)


main()
